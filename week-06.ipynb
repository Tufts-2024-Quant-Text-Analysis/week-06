{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To begin, we're going to initialize a pandas DataFrame of Greek tragedy by line.\n",
    "\n",
    "You might be wondering why we aren't using Pausanias as usual. Tragedy has some nice built-in features that let us get to the heart of some experiments more quickly: rather than having broad geographical labels, each line comes pre-labeled by `dramatist`, `play`, and `speaker`.\n",
    "\n",
    "From the `speaker` label, we can derive information such as age, gender, social status, etc.\n",
    "\n",
    "These labels thus provide a lot of categorical data essentially for free, giving us a number of variables with which to experiment.\n",
    "\n",
    "I've gone ahead and pre-processed the Perseus XML for you, although you should take a look at [`preprocess.py`](./preprocess.py) when you have a chance so that you know what's happening.\n",
    "\n",
    "Let's load up the dataframe and confirm that things look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_pickle(\"./greek-tragedy-by-line.pickle\")\n",
    "\n",
    "df[df['speaker'].str.lower() == 'chorus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Before we get into the details of TF-IDF --- a somewhat old-school method that still deserves attention --- let's get a feel for what its results look like.\n",
    "\n",
    "We'll divide the lines by dramatist and collapse all of the rows per dramatist into three very long strings. These strings are the **documents** that make up our **corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeschylus_text = ' '.join(df[df['dramatist'] == 'Aeschylus']['text'].astype(str))\n",
    "sophocles_text = ' '.join(df[df['dramatist'] == 'Sophocles']['text'].astype(str))\n",
    "euripides_text = ' '.join(df[df['dramatist'] == 'Euripides']['text'].astype(str))\n",
    "\n",
    "corpus = [aeschylus_text, sophocles_text, euripides_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we're going to use the `TfidfVectorizer` class from `scikit-learn` to calculate the TF-IDF scores for each term in the corpus, labeled by document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    index=[\"Aeschylus\", \"Sophocles\", \"Euripides\"],\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have only 3 rows but 12,121 columns -- a bit unwieldy. Let's pick a selection of words to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"apollo\",\n",
    "    \"death\", \n",
    "    \"delphi\",\n",
    "    \"divinity\",\n",
    "    \"gods\",\n",
    "    \"humankind\",\n",
    "    \"humans\",\n",
    "    \"life\",\n",
    "    \"men\",\n",
    "    \"sanctuary\",\n",
    "    \"women\",\n",
    "    \"zeus\"\n",
    "]\n",
    "\n",
    "tfidf_df[keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so bad. But what do these numbers tell us?\n",
    "\n",
    "> Discuss: How do you interpret the numbers above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "\n",
    "We have already worked with **term frequency** extensively in this course: it is the raw frequency of a given **term** in the **corpus**.\n",
    "\n",
    "What is a **term** in this case? It could be a word, a token, or a lemma, or it could be a lexico-grammatical or syntactic feature. We use **term** as a flexible catch-all for any countable feature of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Reades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian, August. https://programminghistorian.org/en/lessons/clustering-visualizing-word-embeddings.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
