{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression â€” Brezina ch. 4 + Google's Crash Course in Machine Learning\n",
    "\n",
    "Because Brezina's explanations are sparse (no pun intended), we're going to lean on Google's Crash Course in Machine Learning to talk about Logistic Regression.\n",
    "\n",
    "But first, we'll need to look at [linear regression](https://developers.google.com/machine-learning/crash-course/linear-regression).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "Measures the difference between predicted and actual values -- focusing on distance, not direction\n",
    "\n",
    "- Mean Squared Error - square of difference (*does* move model towards outliers)\n",
    "- Mean Absolute Error - absolute value (does not move model towards outliers)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "## Learning rate\n",
    "\n",
    "- Too low: model takes a long time to converge\n",
    "- Too high: model never converges, but bounces around the weights and biases that minimize loss\n",
    "\n",
    "\n",
    "## Batch size\n",
    "\n",
    "Number of examples the model processes before updating its weights and bias\n",
    "\n",
    "### Stochastic gradient (SGD)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "> Given enough iterations, SGD works but is very noisy. \"Noise\" refers to variations during training that cause the loss to increase rather than decrease during an iteration. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "\n",
    "\n",
    "### Mini-batch stochastic gradient descent (mini-batch SGD)\n",
    "\n",
    "For N data points, the batch size can be any number > 1 and < N\n",
    "\n",
    "\n",
    "## Epoch\n",
    "\n",
    "\"means that the model has processed every example in the training set _once_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [2.5, 3, 3.5, 6, 4.5, 5, 5.5, 4, 6.5, 7]\n",
    "y_pred = [2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7]\n",
    "\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Essentially a way of getting predictive output from a linear mmodel.\n",
    "\n",
    "## Sigmoid function\n",
    "\n",
    "Standard logistic function for this kind of work.\n",
    "\n",
    "```math\n",
    "f(x) = \\frac{1}{1+e^{-x}}\n",
    "```\n",
    "\n",
    "## z\n",
    "\n",
    "z is the output of the linear equation, also called the \"log odds.\"\n",
    "\n",
    "```math\n",
    "y = \\frac{1}{1 + e^{-z}}\n",
    "```\n",
    "\n",
    "solving for z gives:\n",
    "\n",
    "```math\n",
    "z = log{\\frac{y}{1 - y}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss\n",
    "\n",
    "```math\n",
    "Log Loss = \\sum_{(x, y) \\in D} -y \\log{y'} - (1 - y) \\log(1 - y')\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- $(x, y) \\in D$ is the dataset containing many labeled exaples, which are $(x, y)$ pairs.\n",
    "- $y$ is the label in a lableed example. Since this is logistic regression, every value of $y$ must be either 0 or 1.\n",
    "- $y'$ is your model's prediction (0 < y' < 1), given the set of features in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "- Penalizes model complexity during training\n",
    "\n",
    "### L<sub>2</sub> regularization\n",
    "\n",
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, add code cells and experiment as needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
